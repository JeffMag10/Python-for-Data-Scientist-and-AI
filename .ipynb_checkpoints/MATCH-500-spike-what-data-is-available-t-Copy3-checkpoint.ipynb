{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Jeff Magouirk\n",
    "<br>Date : 09/10/2020\n",
    "<br>Confluence page - https://confluence.dhigroupinc.com/display/MATCH/MATCH-500-spike-what-data-is-available-t\n",
    "<br>Jira Page:https://jira.dhigroupinc.com/browse/MATCH-500\n",
    "<br>BitBucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip\n",
    "#!pip install --upgrade jsonpath-ng\n",
    "#!pip install --upgrade pandas\n",
    "#!pip install sagemaker\n",
    "#!pip install boto3\n",
    "#!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import boto3\n",
    "from itertools import chain\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing as mp\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from jsonpath_ng import jsonpath, parse\n",
    "\n",
    "from dsmatch import local_bucket,s3_ds_bucket\n",
    "from dsmatch.util.parallel import get_n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Bringing in the parsed Burning Glass files of job descriptions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_jd = os.path.join(local_bucket, 'data', 'efc', 'jobseeker-applies','job_descriptions')#Calling parsed jd by BG\n",
    "folders = [x for x in os.listdir(input_dir_jd) if not x.endswith('.csv')]#calling the folders insde the jobseeker-apples\n",
    "output_dir_jd='bg-jd-skill-names' # output dir \n",
    "output_path_jd = os.path.join(local_bucket, 'data', 'efc', 'jobseeker-applies', output_dir_jd)#output path\n",
    "\n",
    "total_jd=[]#making an object\n",
    "for folder in folders:\n",
    "#     already_processed_names = [f.split('/')[-1][:-4] for f in os.listdir(output_path_jd) if f.endswith('.csv')]#\n",
    "    filelist=[os.path.join(input_dir_jd, folder, x)\\\n",
    "              for x in os.listdir(os.path.join(input_dir_jd, folder)) \\\n",
    "#               if x.endswith(\".json\") and x[:-5] not in already_processed_names\n",
    "             ]\n",
    "    total_jd.extend(filelist)\n",
    "\n",
    "print(\"Number of files need to process\",len(total_jd))\n",
    "#print(\"Number Processed files\",len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Defined function to read in the filelist </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(filelist):\n",
    "    records = []\n",
    "    for filename in filelist:\n",
    "        with open(filename, 'r') as f:\n",
    "            d = json.load(f)\n",
    "            jsonpath_expr = parse(f'$..skillrollup[*].canonskill[*].variant[*]')\n",
    "            matches = [match.value for match in jsonpath_expr.find(d)]\n",
    "            if len(matches) > 0:        \n",
    "                splitted = filename.split('/')\n",
    "                date_name = splitted[-2]\n",
    "                job_application_id = splitted[-1][:-5]\n",
    "                records.append((date_name, job_application_id, matches))\n",
    "    return records\n",
    "            \n",
    "max_workers = mp.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the records for multiple processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits = get_n_splits(total_jd, chunksize=.05, min_chunksize=10)\n",
    "\n",
    "records = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    args_list = [X for X in np.array_split(total_jd, n_splits)]\n",
    "    records.extend(chain(*tqdm(executor.map(process_files, args_list), total=len(args_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Looking at the output of date_name, job_application_id, parsed data(variant) </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_0 = pd.DataFrame(records)\n",
    "df_0.info()\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output of the variant field where the the languages are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_0.iloc[2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Finding the languages in the parsed job description field variant from Burning Glass </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_jd = pd.DataFrame(records, columns=['date', 'job_application_id', 'variant'])\n",
    "\n",
    "df_jd = df_jd.explode('variant')\n",
    "df_jd.drop_duplicates(inplace=True)\n",
    "\n",
    "df_jd['variant'] = df_jd['variant'].str.lower()\n",
    "\n",
    "###Top 20 langugaes spoken in the world, by number of estimated speakers\n",
    "\n",
    "L = ['arabic','english','chinese','japanese','mandarin','hinidi','spanish','french','bengali','russian',\n",
    "    'portuguese','urdu','german','indonesian','swahili','marathi','telugu','turkish','cantonese','tamil',\n",
    "    'western punjabi','punjabi']\n",
    "\n",
    "df_jd[\"Required_language\"]= df_jd['variant'].str.contains('|'.join(L), flags=re.I)\n",
    "\n",
    "print('Required Language = \\n',df_jd['Required_language'].value_counts())\n",
    "df_language = pd.DataFrame(df_jd[df_jd['Required_language']==True])\n",
    "print('Variant = \\n',df_language['variant'].value_counts().sort_values(ascending =False))\n",
    "df_cnts = pd.DataFrame(df_language['job_application_id'].value_counts().sort_values(ascending=False))\n",
    "print('Counts of Languages = \\n',df_cnts['job_application_id'].value_counts().sort_values(ascending=False))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_language.info()\n",
    "df_language = df_language.rename(columns={'job_application_id':'job_application.data.id'})\n",
    "df_language.head(n=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Looking at Language the Resume is written </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug = pd.read_csv('s3://dev-dhi-match-datascience/data/efc/live-feed/raw-20200826.csv')\n",
    "df_aug.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Exploding out the languagues and language competencies of the job seekers</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = df_aug[['jobseeker.data.id','jobseeker.data.languages','job_application.data.job_id','job_application.data.id']]\n",
    "t2 = t2.dropna()\n",
    "t2['jobseeker.data.languages'] = t2['jobseeker.data.languages'].apply(eval)\n",
    "t3 = t2.explode('jobseeker.data.languages') \n",
    "t3 = t3.dropna()\n",
    "t3['language'] = t3['jobseeker.data.languages'].apply(lambda x: x['language'])\n",
    "t3['language_competency'] = t3['jobseeker.data.languages'].apply(lambda x: \n",
    "                                                        (x['language'],x['language_competency']))\n",
    "print(t3.shape)\n",
    "t3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Merging the language of the resume with the langugage requirement on the job description</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug1 = df_aug[['job_application.data.id','Language_JD','Language_Resume']]\n",
    "df_merge = df_language.merge(df_aug1,on='job_application.data.id', how='left')\n",
    "df_merge[\"Language_Resume\"].replace({\"en\": \"english\", \"fr\": \"french\",\"de\":\"german\",\n",
    "                                    'zh-cn':'chinese-prc','cy':'welsh',\n",
    "                                    'it':'italian','ca':'catalan','ro':'romanian',\n",
    "                                    'ar':'arabic','es':'spanish','nl':'dutch'}, inplace=True)\n",
    "df_merge.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df_merge[['job_application.data.id']]\n",
    "print('Id counts =',counts.shape)\n",
    "counts = counts.drop_duplicates()\n",
    "print('Unique Id counts =',counts.shape)\n",
    "df_merge1 = df_merge[df_merge['variant']==df_merge['Language_Resume']]\n",
    "print(df_merge1.shape)\n",
    "df_cnts = df_merge1.variant.value_counts().sort_values(ascending=False)\n",
    "print(df_cnts.head())\n",
    "df_merge1.head()\n",
    "df_merge2 = df_merge1.drop_duplicates()\n",
    "df_merge2.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
